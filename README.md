# AlgorithmicBias
A Research Analysis on AI Algorithmic Bias
Written by Arash Dewan

Artificial intelligence, with its foundation in mathematics and logistics, is leading us closer to the future through the capabilities of algorithms and machine learning procedures. However, the lack of transparency and understanding of these ‘black boxes’ is becoming a growing concern as they gain more control over our daily lives. Research and studies have raised questions about the algorithmic bias embedded in these datasets and how they can unfairly impact individuals. The utilization of these algorithms are even predominant within the criminal justice system—determining bail, sentencing, criminal identification, along with various other contributions. Within the findings, it is notably evident that computational algorithms have a disproportionate bias that goes beyond chance and instead is contributing to a world of automated racism.

Within courtrooms across the United States, the expectations of artificial intelligence are high in demand. To determine the decision whether to send a defendant home or to jail as they await trial, is based entirely on specialized algorithms. It takes in risk assessment tools such as identity, history, demographics, along with other details to formulate a score quantifying how likely the defendant is to commit another crime or appear at the following hearing; these assessments are presented as ‘race-neutral’ and solely off objective criteria of science (Schwartzapfel). Although the efficiency of these results can be tremendously beneficial in assistance, a study conducted by ProPublica in 2016 discovered black defendants were almost twice as likely as white defendants to be “false positive”, meaning inadequately labeled as high risk for committing another crime in contrast to white defendants. Outside of the courtroom, these algorithms are used for law enforcement within the criminal justice system. UK Police have begun running trials with biometric identity checks along the streets to track criminals and predict crime. There were countless flaws with incorrectly matching an innocent person to a wanted person (Kantayya). These results of utilizing facial recognition in law enforcement are harmful in a multitude of variations, and can alter the lives of innocent individuals. 

A large reason for these biases that algorithms form are due to the poorly designed datasets and systemic faults within society. When trying to build a machine learning prediction model, it begins with inputting large groups of quantitative information and measurements; the programmers may not explicitly include race or gender as inputs, but machine learning excels at picking up these kinds of latent variables and encoding it (Thomas). Trying to remain objectively neutral and not regulate these “socio-cultural and demographic factors can result in an adversarial impact [that] can exaggerate harm to disadvantaged populations of different social status, subcultures, gender, and other social groups” (Akter). Failure to properly configure how AI interprets the data is exactly what results in improper conclusions within the criminal justice system.

This form of AI is not developed for public interest. It is all about power—whoever owns the code, does it for commercial needs and revenue. Having individuals’ lives be altered and misrepresented solely off algorithms is a cruel idea. “The adverse results that humans suffer at the hands of automated processes that are data-driven leading to unjust or unfair rulings” is caused by this algorithmic fallout (Akter). The criminal justice system has begun implementing predictive policing tools that take in location-based and person-based algorithms to draw in areas of crime as well as make determinations of civilians. When pushing more police towards areas that the algorithm presents, it only causes a feedback loop of more individuals in that area being arrested. And the second someone is put into handcuffs, it progresses to the courtroom where they can use a tool such as COMPAS which issues a statistical score between 1 and 10 to quantify how likely a person is to be rearrested if released (Heaven). As previously discussed, black individuals are twice as likely to be incorrectly predicted as high-risk, which goes to show this algorithmic fallout at play. It seems inhumane to create such kinds of algorithms that have evidently been shown to negatively influence black individuals, where they now have to live a life with a record, only feeding back into the algorithmic loop of faulty predictions. 

There is no denying that artificial intelligence will be the future of society. It presents us with a whole new array of technology that embodies what science fiction once wrote about. The capabilities of these algorithms are already in our everyday lives, and it will only get greater. But beyond scanning one’s face to unlock their phone or websites providing individuals with personalized ads, the use of AI in larger scales holds unforeseen underlying consequences. The criminal justice system is a crucial aspect of society, and requires far more regulation and ethics control when producing these algorithms. Mandating the source of datasets, recognizing the intended use of the algorithm, and evaluating performance on different subgroups are simple but necessary steps to be tested for this form of bias to be dismantled (Thomas). By finding programmers with unique and diverse perspectives who allow for further insight on socio-cultural and economic factors that may be latent variables in the data would allow for more accurate datasets. There is bound to be much more work conducted to comprehend these black boxes of prediction, but by incorporating ethical and social ideas into the process of data science procedures, it will create a future of safer and quite frankly smarter code. 
